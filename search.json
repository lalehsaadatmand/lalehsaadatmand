[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Laleh Saadatmand",
    "section": "",
    "text": "I am pursuing master of Human Computer Interactionüë©‚Äçüéìüíª"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am going to write about my self here."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "I am going to write about my research here."
  },
  {
    "objectID": "coursework.html",
    "href": "coursework.html",
    "title": "Coursework",
    "section": "",
    "text": "I am going to show the coursework here.\n*Assignment 1(Data Visualization)*\nQ1.\n\n## Data Visualization\n## Objective: Identify data or model problems using visualization\n## Anscombe (1973) Quartlet\n\ndata(anscombe)  # Load Anscombe's data\nView(anscombe) # View the data\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n## Simple version\nplot(anscombe$x1,anscombe$y1)\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n# Create four model objects\nlm1 &lt;- lm(y1 ~ x1, data=anscombe)\nsummary(lm1)\n\n\nCall:\nlm(formula = y1 ~ x1, data = anscombe)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.92127 -0.45577 -0.04136  0.70941  1.83882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0001     1.1247   2.667  0.02573 * \nx1            0.5001     0.1179   4.241  0.00217 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6665,    Adjusted R-squared:  0.6295 \nF-statistic: 17.99 on 1 and 9 DF,  p-value: 0.00217\n\nlm2 &lt;- lm(y2 ~ x2, data=anscombe)\nsummary(lm2)\n\n\nCall:\nlm(formula = y2 ~ x2, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9009 -0.7609  0.1291  0.9491  1.2691 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)    3.001      1.125   2.667  0.02576 * \nx2             0.500      0.118   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6662,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002179\n\nlm3 &lt;- lm(y3 ~ x3, data=anscombe)\nsummary(lm3)\n\n\nCall:\nlm(formula = y3 ~ x3, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1586 -0.6146 -0.2303  0.1540  3.2411 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0025     1.1245   2.670  0.02562 * \nx3            0.4997     0.1179   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6663,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002176\n\nlm4 &lt;- lm(y4 ~ x4, data=anscombe)\nsummary(lm4)\n\n\nCall:\nlm(formula = y4 ~ x4, data = anscombe)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.751 -0.831  0.000  0.809  1.839 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0017     1.1239   2.671  0.02559 * \nx4            0.4999     0.1178   4.243  0.00216 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6667,    Adjusted R-squared:  0.6297 \nF-statistic:    18 on 1 and 9 DF,  p-value: 0.002165\n\nplot(anscombe$x1,anscombe$y1)\nabline(coefficients(lm1))\n\n\n\n\n\n\n\nplot(anscombe$x2,anscombe$y2)\nabline(coefficients(lm2))\n\n\n\n\n\n\n\nplot(anscombe$x3,anscombe$y3)\nabline(coefficients(lm3))\n\n\n\n\n\n\n\nplot(anscombe$x4,anscombe$y4)\nabline(coefficients(lm4))\n\n\n\n\n\n\n\n## Fancy version (per help file)\n\nff &lt;- y ~ x\nmods &lt;- setNames(as.list(1:4), paste0(\"lm\", 1:4))\n\n# Plot using for loop\nfor(i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  ## or   ff[[2]] &lt;- as.name(paste0(\"y\", i))\n  ##      ff[[3]] &lt;- as.name(paste0(\"x\", i))\n  mods[[i]] &lt;- lmi &lt;- lm(ff, data = anscombe)\n  print(anova(lmi))\n}\n\nAnalysis of Variance Table\n\nResponse: y1\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nx1         1 27.510 27.5100   17.99 0.00217 **\nResiduals  9 13.763  1.5292                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y2\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx2         1 27.500 27.5000  17.966 0.002179 **\nResiduals  9 13.776  1.5307                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y3\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx3         1 27.470 27.4700  17.972 0.002176 **\nResiduals  9 13.756  1.5285                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y4\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx4         1 27.490 27.4900  18.003 0.002165 **\nResiduals  9 13.742  1.5269                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsapply(mods, coef)  # Note the use of this function\n\n                  lm1      lm2       lm3       lm4\n(Intercept) 3.0000909 3.000909 3.0024545 3.0017273\nx1          0.5000909 0.500000 0.4997273 0.4999091\n\nlapply(mods, function(fm) coef(summary(fm)))\n\n$lm1\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0000909  1.1247468 2.667348 0.025734051\nx1          0.5000909  0.1179055 4.241455 0.002169629\n\n$lm2\n            Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.000909  1.1253024 2.666758 0.025758941\nx2          0.500000  0.1179637 4.238590 0.002178816\n\n$lm3\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0024545  1.1244812 2.670080 0.025619109\nx3          0.4997273  0.1178777 4.239372 0.002176305\n\n$lm4\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0017273  1.1239211 2.670763 0.025590425\nx4          0.4999091  0.1178189 4.243028 0.002164602\n\n# Preparing for the plots\nop &lt;- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma =  c(0, 0, 2, 0))\n\n# Plot charts using for loop\nfor(i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  plot(ff, data = anscombe, col = \"red\", pch = 21, bg = \"orange\", cex = 1.2,\n       xlim = c(3, 19), ylim = c(3, 13))\n  abline(mods[[i]], col = \"blue\")\n}\nmtext(\"Anscombe's 4 Regression data sets\", outer = TRUE, cex = 1.5)\n\n\n\n\n\n\n\npar(op)\n\nQ3.\n\n## Data Visualization\n## Objective: Create graphics with R\n## Title: Fall color\n# Credit: https://fronkonstin.com\n\n# Install packages\noptions(repos = c(CRAN = \"https://cran.rstudio.com/\"))\ninstall.packages(c(\"gsubfn\", \"proto\", \"tidyverse\"))\n\nInstalling packages into 'C:/Users/zibae/AppData/Local/R/win-library/4.4'\n(as 'lib' is unspecified)\n\n\npackage 'gsubfn' successfully unpacked and MD5 sums checked\npackage 'proto' successfully unpacked and MD5 sums checked\npackage 'tidyverse' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\zibae\\AppData\\Local\\Temp\\RtmpApPPXl\\downloaded_packages\n\nlibrary(gsubfn)\n\nLoading required package: proto\n\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî ggplot2   3.5.1     ‚úî tibble    3.2.1\n‚úî lubridate 1.9.3     ‚úî tidyr     1.3.1\n‚úî purrr     1.0.2     \n\n\n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Define elements in plant art\n# Each image corresponds to a different axiom, rules, angle and depth\n\n# Leaf of Fall\n\naxiom=\"X\"\nrules=list(\"X\"=\"F-[[X]+X]+F[+FX]-X\", \"F\"=\"FF\")\nangle=22.5\ndepth=6\n\n\nfor (i in 1:depth) axiom=gsubfn(\".\", rules, axiom)\n\nactions=str_extract_all(axiom, \"\\\\d*\\\\+|\\\\d*\\\\-|F|L|R|\\\\[|\\\\]|\\\\|\") %&gt;% unlist\n\nstatus=data.frame(x=numeric(0), y=numeric(0), alfa=numeric(0))\npoints=data.frame(x1 = 0, y1 = 0, x2 = NA, y2 = NA, alfa=90, depth=1)\n\n\n# Generating data\n# Note: may take a minute or two\n\nfor (action in actions)\n{\n  if (action==\"F\")\n  {\n    x=points[1, \"x1\"]+cos(points[1, \"alfa\"]*(pi/180))\n    y=points[1, \"y1\"]+sin(points[1, \"alfa\"]*(pi/180))\n    points[1,\"x2\"]=x\n    points[1,\"y2\"]=y\n    data.frame(x1 = x, y1 = y, x2 = NA, y2 = NA,\n               alfa=points[1, \"alfa\"],\n               depth=points[1,\"depth\"]) %&gt;% rbind(points)-&gt;points\n  }\n  if (action %in% c(\"+\", \"-\")){\n    alfa=points[1, \"alfa\"]\n    points[1, \"alfa\"]=eval(parse(text=paste0(\"alfa\",action, angle)))\n  }\n  if(action==\"[\"){\n    data.frame(x=points[1, \"x1\"], y=points[1, \"y1\"], alfa=points[1, \"alfa\"]) %&gt;%\n      rbind(status) -&gt; status\n    points[1, \"depth\"]=points[1, \"depth\"]+1\n  }\n\n  if(action==\"]\"){\n    depth=points[1, \"depth\"]\n    points[-1,]-&gt;points\n    data.frame(x1=status[1, \"x\"], y1=status[1, \"y\"], x2=NA, y2=NA,\n               alfa=status[1, \"alfa\"],\n               depth=depth-1) %&gt;%\n      rbind(points) -&gt; points\n    status[-1,]-&gt;status\n  }\n}\n\nggplot() +\n  geom_segment(aes(x = x1, y = y1, xend = x2, yend = y2),\n               lineend = \"round\",\n               color=\"burlywood3\", # Set your own Fall color?\n               data=na.omit(points)) +\n  coord_fixed(ratio = 1) +\n  theme_void() # No grid nor axes\n\n\n\n\n\n\n\n\n*Assignment 2 (Data Visualization) *\nQ1.\n\n### Paul Murrell's R examples (selected)\nhead(pressure)\n\n  temperature pressure\n1           0   0.0002\n2          20   0.0012\n3          40   0.0060\n4          60   0.0300\n5          80   0.0900\n6         100   0.2700\n\n## Start plotting from basics \n# Note the order\nplot(pressure, pch=20) # Can you change pch?\ntext(150, 600, \n     \"Pressure (mm Hg)\\nversus\\nTemperature (Celsius)\")\n\n\n\n\n\n\n\n#  Examples of standard high-level plots \n#  In each case, extra output is also added using low-level \n#  plotting functions.\n# \n\n# Setting the parameter (3 rows by 2 cols)\npar(mfrow=c(3, 2))\n\n# Scatterplot\n# Note the incremental additions\n\nx &lt;- c(0.5, 2, 4, 8, 12, 16)\ny1 &lt;- c(1, 1.3, 1.9, 3.4, 3.9, 4.8)\ny2 &lt;- c(4, .8, .5, .45, .4, .3)\n\n# Setting label orientation, margins c(bottom, left, top, right) & text size\npar(las=1, mar=c(4, 4, 2, 4), cex=.7) \nplot.new()\nplot.window(range(x), c(0, 6))\nlines(x, y1)\nlines(x, y2)\npoints(x, y1, pch=16, cex=0.7) # Try different cex value?  \npoints(x, y2, pch=21, bg=\"blue\", cex=0.7)  # Different background color\npar(col=\"gray50\", fg=\"gray50\", col.axis=\"gray50\")\naxis(1, at=seq(0, 16, 4)) # What is the first number standing for? x axis\naxis(2, at=seq(0, 6, 2)) # y axis\naxis(4, at=seq(0, 6, 2))\nbox(bty=\"u\")\nmtext(\"Travel Time (s)\", side=1, line=2, cex=0.5)\nmtext(\"Responses per Travel\", side=2, line=2, las=0, cex=0.5)\nmtext(\"Responses per Second\", side=4, line=2, las=0, cex=0.5)\ntext(4, 5, \"Bird 131\")\npar(mar=c(5.1, 4.1, 4.1, 2.1), col=\"black\", fg=\"black\", col.axis=\"black\")\n\n# Histogram\n# Random data\nY &lt;- rnorm(50)\n# Make sure no Y exceed [-3.5, 3.5]\nY[Y &lt; -3.5 | Y &gt; 3.5] &lt;- NA # Selection/set range\nx &lt;- seq(-3.5, 3.5, .1)\ndn &lt;- dnorm(x)\npar(mar=c(4.5, 4.1, 3.1, 0))\nhist(Y, breaks=seq(-3.5, 3.5), ylim=c(0, 0.5), \n     col=\"lightgreen\", freq=FALSE)\nlines(x, dnorm(x), lwd=2)\npar(mar=c(5.1, 4.1, 4.1, 2.1))\n\n# Barplot\npar(mar=c(2, 3.1, 2, 2.1)) \nmidpts &lt;- barplot(VADeaths, \n                  col=gray(0.1 + seq(1, 9, 2)/11), \n                  names=rep(\"\", 4))\nmtext(sub(\" \", \"\\n\", colnames(VADeaths)),\n      at=midpts, side=1, line=0.5, cex=0.5)\ntext(rep(midpts, each=5), apply(VADeaths, 2, cumsum) - VADeaths/2,\n     VADeaths, \n     col=rep(c(\"white\", \"black\"), times=3:2), \n     cex=0.8)\npar(mar=c(5.1, 4.1, 4.1, 2.1))  \n\n# Boxplot\npar(mar=c(3, 4.1, 2, 0))\nboxplot(len ~ dose, data = ToothGrowth,\n        boxwex = 0.25, at = 1:3 - 0.2,\n        subset= supp == \"VC\", col=\"white\",\n        xlab=\"\",\n        ylab=\"tooth length\", ylim=c(0,35))\nmtext(\"Vitamin C dose (mg)\", side=1, line=2.5, cex=0.8)\nboxplot(len ~ dose, data = ToothGrowth, add = TRUE,\n        boxwex = 0.25, at = 1:3 + 0.2,\n        \n        subset= supp == \"OJ\")\nlegend(1.5, 11, c(\"Ascorbic acid\", \"Orange juice\"), \n       fill = c(\"white\", \"gray\"), \n       bty=\"n\")\npar(mar=c(5.1, 4.1, 4.1, 2.1))\n\n# Persp\nx &lt;- seq(-10, 10, length= 30)\ny &lt;- x\nf &lt;- function(x,y) { r &lt;- sqrt(x^2+y^2); 10 * sin(r)/r }\nz &lt;- outer(x, y, f)\nz[is.na(z)] &lt;- 1\n# 0.5 to include z axis label\npar(mar=c(0, 0.5, 0, 0), lwd=0.5)\npersp(x, y, z, theta = 30, phi = 30, \n      expand = 0.5)\npar(mar=c(5.1, 4.1, 4.1, 2.1), lwd=1)\n\n# Piechart\npar(mar=c(0, 2, 1, 2), xpd=FALSE, cex=0.5)\npie.sales &lt;- c(0.12, 0.3, 0.26, 0.16, 0.04, 0.12)\nnames(pie.sales) &lt;- c(\"Blueberry\", \"Cherry\",\n                      \"Apple\", \"Boston Cream\", \"Other\", \"Vanilla\")\npie(pie.sales, col = gray(seq(0.3,1.0,length=6))) \n\n\n\n\n\n\n\n\nAssignment 3(Data Visualization)\n\n# Histogram\n\n# Step 1: Generate random data\nY &lt;- rnorm(50,0,1) # Generate 50 random numbers from a standard normal distribution (mean = 0, sd = 1)\n\n# Step 2: Handling outliers\nY[Y &lt; -3.5 | Y &gt; 3.5] &lt;- NA # Replace values in Y that are outside the range [-3.5, 3.5] with NA\n\n# Step 3: Set x-values for the normal distribution curve\nx &lt;- seq(-3.5, 3.5, .1) # Create a sequence of x-values from -3.5 to 3.5, with an interval of 0.1\n\n# Step 4: Calculate the normal density for the x-values\ndn &lt;- dnorm(x) # Get the probability density function values of the normal distribution for each x\n\n\n# Step 5: Create the histogram\nhist(Y, \n     breaks=seq(-3.5, 3.5), # Breaks of the histogram are set to match the range of the data\n     ylim=c(0, 0.5),        # Set the y-axis limits for the histogram to go from 0 to 0.5\n     col=\"#00abff\",           # Color the histogram bars light gray\n     freq=FALSE)             # Display the histogram with densities (not frequencies)\n\n# Step 6: Overlay the normal distribution curve\nlines(x, dnorm(x), lwd=2)    # Draw a line for the normal density over the histogram, with a line width of 2\n\n\n\n\n\n\n\n\nQ2. a)The descriptive statistics of all four regression models generated using Anscombe‚Äôs quartet data point to the fact that the regression coefficients are the same as well as the p-values and the adjusted R-squared were values in the 1, 2, 3, and 4 linear models. Furthermore, four models, lm1, lm2, lm3, and lm4, are almost the same in this statistical matter, which implies nearly 63% of the variation in each dependent variable (y1, y2, y3, y4) is accounted for by the corresponding independent variables (x1, x2, x3, x4). This is a matter of the fact that such an analysis has been made without considering the different underlying distributions,which can be revealed only by visual means.\n\n\n\n\n## Data Visualization\n## Objective: Identify data or model problems using visualization\n## Anscombe (1973) Quartlet\n\ndata(anscombe)  # Load Anscombe's data\nView(anscombe) # View the data\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n## Simple version\nplot(anscombe$x1,anscombe$y1)\n\n\n\n\n\n\n\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n# Create four model objects\n# Using x1 as the independent variable and y1 as dependent variable\nlm1 &lt;- lm(y1 ~ x1, data=anscombe)\n# Coefficient of x1 is 0.5001 and intercept y1 is 3.0001. \n# The over all p-value(0.00217) is &lt;= 0.05 that means the model is significant.\n# The Adjusted R-squared:0.6295 that means the overall model explain around 62% of variation in the data.\nsummary(lm1)\n\n\nCall:\nlm(formula = y1 ~ x1, data = anscombe)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.92127 -0.45577 -0.04136  0.70941  1.83882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0001     1.1247   2.667  0.02573 * \nx1            0.5001     0.1179   4.241  0.00217 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6665,    Adjusted R-squared:  0.6295 \nF-statistic: 17.99 on 1 and 9 DF,  p-value: 0.00217\n\n# Using x2 as the independent variable and y2 as dependent variable\nlm2 &lt;- lm(y2 ~ x2, data=anscombe)\n# Coefficient of x2 is 0.500 and intercept y2 is 3.001. \n# The over all p-value(0.002179) is &lt;= 0.05 that means the model is significant.\n# The Adjusted R-squared:0.6292 that means the overall model explain around 62.92% of variation in the data.\nsummary(lm2)\n\n\nCall:\nlm(formula = y2 ~ x2, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9009 -0.7609  0.1291  0.9491  1.2691 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)    3.001      1.125   2.667  0.02576 * \nx2             0.500      0.118   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6662,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002179\n\n# Using x3 as the independent variable and y3 as dependent variable\nlm3 &lt;- lm(y3 ~ x3, data=anscombe)\n# Coefficient of x3 is 0.4997 and intercept y3 is 3.0025. \n# The over all p-value(0.002176) is &lt;= 0.05 that means the model is significant.\n# The Adjusted R-squared:0.6292 that means the overall model explain around 62.92% of variation in the data.\nsummary(lm3)\n\n\nCall:\nlm(formula = y3 ~ x3, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1586 -0.6146 -0.2303  0.1540  3.2411 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0025     1.1245   2.670  0.02562 * \nx3            0.4997     0.1179   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6663,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002176\n\n# Using x4 as the independent variable and y4 as dependent variable\nlm4 &lt;- lm(y4 ~ x4, data=anscombe)\n# Coefficient of x4 is 0.4999 and intercept y4 is 3.0017. \n# The over all p-value(0.002165) is &lt;= 0.05 that means the model is significant.\n# The Adjusted R-squared:0.6297 that means the overall model explain around 62.97% of variation in the data.\nsummary(lm4)\n\n\nCall:\nlm(formula = y4 ~ x4, data = anscombe)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.751 -0.831  0.000  0.809  1.839 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0017     1.1239   2.671  0.02559 * \nx4            0.4999     0.1178   4.243  0.00216 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6667,    Adjusted R-squared:  0.6297 \nF-statistic:    18 on 1 and 9 DF,  p-value: 0.002165\n\n# Plot for x1 y1 relationship\nplot(anscombe$x1,anscombe$y1,pch = 20, col = \"blue\")\nabline(coefficients(lm1), col = \"red\",lwd = 2)\n\n\n\n\n\n\n\n# Plot for x2 y2 relationship\nplot(anscombe$x2,anscombe$y2,pch = 16, col = \"darkgreen\")\nabline(coefficients(lm2),lwd = 1)\n\n\n\n\n\n\n\n# Plot for x3 y3 relationship\nplot(anscombe$x3,anscombe$y3,pch = 21, col = \"purple\")\nabline(coefficients(lm3), col = \"red\",lwd = 3)\n\n\n\n\n\n\n\n# Plot for x4 y4 relationship\nplot(anscombe$x4,anscombe$y4,pch = 18, col = \"coral4\")\nabline(coefficients(lm4),lwd = 2)\n\n\n\n\n\n\n\n\nQ3.\n\n# using Women data \nView(women)\n# Set the plotting parameters for the entire graphics device\npar(family = \"serif\", bg = \"lightgrey\")  # Use a serif font and light grey background\n# First dataset plot\nplot(women$weight, women$height,\n     main = \"Women\",\n     xlab = \"weight(lbs)\",\n     ylab = \"height(in)\",\n     col = \"steelblue\",   # Custom color for points\n     pch = 19,            # Filled circle\n     cex = 1.5)           # Adjust size of points\nabline(lm(height ~ weight, data = women), col = \"tomato\")  # Custom color for regression line\n\n\n\n\n\n\n\n\nQ4.\n\n# Install the tidyverse package if needed\ninstall.packages(\"tidyverse\")\n\nWarning: package 'tidyverse' is in use and will not be installed\n\n# Load the tidyverse package\nlibrary(tidyverse)\n\n# Create the scatterplot with a regression line\nggplot(data = trees, aes(x = Girth, y = Height)) +\n  geom_point(color = \"blue\", size = 3, na.rm = TRUE) +        # Scatterplot with custom point color and size\n  geom_smooth(method = \"lm\", color = \"forestgreen\", na.rm = TRUE) +  # Linear regression line with custom color\n  labs(title = \" Girth vs Height with Regression Line\", \n       x = \"Girth (inc)\", y = \"Height (ft)\") +\n  theme_minimal()  # A clean theme\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nAssignment 4(Data Visualization)\n\n# Install necessary packages\ninstall.packages(\"ggplot2\")\n\nWarning: package 'ggplot2' is in use and will not be installed\n\nlibrary(ggplot2)\n\n# Sample data with variable width and height\ndata &lt;- data.frame(\n  category = c(\"A\", \"B\", \"C\", \"D\"),\n  values = c(15, 10, 20, 08),\n  widths = c(0.5, 0.8, 0.5, 0.8)  # Variable widths for each bar\n)\n\n# Plot the variable-width column chart\nggplot(data, aes(x = category, y = values)) +\n  geom_col(aes(width = widths), fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Variable Width Column Chart\", x = \"Category\", y = \"Values\") +\n  theme_minimal()\n\nWarning in geom_col(aes(width = widths), fill = \"skyblue\", color = \"black\"):\nIgnoring unknown aesthetics: width\n\n\n\n\n\n\n\n\n\n\n\n\n\n### Paul Murrell's R examples (selected)\n\npar(mfrow = c(3,2))\nx &lt;- c(0.5, 2, 4, 8, 12, 16)\ny1 &lt;- c(1, 1.3, 1.9, 3.4, 3.9, 4.8)\ny2 &lt;- c(4, .8, .5, .45, .4, .3)\n\npar(las = 1, mar = c(4,4,2,4), cex = 0.7)\nplot.new()\nplot.window( range(x),c(0,6))\n\nlines(x,y1)\nlines(x,y2)\npoints(x,y1,pch = 18, cex = 1,col = \"red\")\npoints(x,y2,pch = 20, cex = 1,col = \"blue\")\npar(col = \"gray50\", fg = \"gray50\",col.axis = \"gray50\" )\naxis(1,at = seq(0,16,4))\naxis(2,at = seq(0,6,2))\naxis(4,at = seq(0,6,2))\nbox(bty = \"U\")\nmtext(\"Travel Time(s)\",side = 1,cex=0.8,line = 2 )\nmtext(\"Responses per Travel\",side = 2,cex=0.8,line = 2, las = 0 )\nmtext(\"Responses per Seconds\",side = 4,cex=0.8,line = 2,las = 0)\npar(mar = c(4,4.1,4.1,2),col = \"black\",fg = \"black\", col.axis = \"black\")\n###################\nY = rnorm(50)\nY[Y &lt; -3.5 | Y &gt; 3.5 ] &lt;- NA\nx &lt;- seq( -3.5, 3.5, 0.1)\ndn &lt;- dnorm(x)\npar(mar = c(3.5, 4.1, 3.1, 0))\nhist(Y, breaks = seq(-3.5,3.5), ylim = c(0, 0.5), col = \"gray80\", freq = FALSE)\nlines(x,dnorm(x),lwd = 2)\npar(mar = c(5.1, 4.1, 4.1, 2.1))\n\n#####################\npar(mar=c(2, 3.1, 2, 2.1))\nmidpts &lt;- barplot(VADeaths, col = gray(0.1 + seq(1,9,2)/11), names = rep(\"\", 4))\nmtext(sub(\" \", \"\\n\", colnames(VADeaths)),\n      at=midpts, side=1, line=0.5, cex=0.5)\ntext(rep(midpts, each=5), apply(VADeaths, 2, cumsum) - VADeaths/2,\n     VADeaths, \n     col=rep(c(\"white\", \"black\"), times=3:2), \n     cex=0.8)\npar(mar=c(5.1, 4.1, 4.1, 2.1))  \n#####################################\n# Piechart\npar(mar=c(0, 2, 1, 2), xpd=FALSE, cex=0.5)\npie.sales &lt;- c(0.12, 0.3, 0.26, 0.16, 0.04, 0.12)\nnames(pie.sales) &lt;- c(\"Blueberry\", \"Cherry\",\n                      \"Apple\", \"Boston Cream\", \"Other\", \"Vanilla\")\npie(pie.sales, col = gray(seq(0.3,1.0,length=6))) \n\n\n\n\n\n\n\n\n\n\n\n\napple_data &lt;- data.frame(Product = c(\"iPhone\", \"iPad\", \"MacBook\", \"AirPods\"),\n                         Price = c(30, 35, 40, 17))\nggplot(apple_data, aes(x = Price, y = Product)) +\n  geom_bar(stat = \"Identity\", fill = \"skyblue\", width = 0.5) +\n  labs(x = \"Price\", y = \"Apple\", title = \"Price of Apple\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\npar(mfrow = c(3,2))\nx &lt;- c(0.5, 2, 4, 8, 12, 16)\ny1 &lt;- c(1, 1.3, 1.9, 3.4, 3.9, 4.8)\ny2 &lt;- c(4, .8, .5, .45, .4, .3)\n\npar(las = 1, mar = c(4,4,2,4), cex = 0.7)\nplot.new()\nplot.window( range(x),c(0,6))\n\nlines(x,y1)\nlines(x,y2)\npoints(x,y1,pch = 18, cex = 1,col = \"red\")\npoints(x,y2,pch = 20, cex = 1,col = \"blue\")\npar(col = \"gray50\", fg = \"gray50\",col.axis = \"gray50\" )\naxis(1,at = seq(0,16,4))\naxis(2,at = seq(0,6,2))\naxis(4,at = seq(0,6,2))\nbox(bty = \"U\")\nmtext(\"Travel Time(s)\",side = 1,cex=0.8,line = 2 )\nmtext(\"Responses per Travel\",side = 2,cex=0.8,line = 2, las = 0 )\nmtext(\"Responses per Seconds\",side = 4,cex=0.8,line = 2,las = 0)\npar(mar = c(4,4.1,4.1,2),col = \"black\",fg = \"black\", col.axis = \"black\")\n###################\nY = rnorm(50)\nY[Y &lt; -3.5 | Y &gt; 3.5 ] &lt;- NA\nx &lt;- seq( -3.5, 3.5, 0.1)\ndn &lt;- dnorm(x)\npar(mar = c(3.5, 4.1, 3.1, 0))\nhist(Y, breaks = seq(-3.5,3.5), ylim = c(0, 0.5), col = \"gray80\", freq = FALSE)\nlines(x,dnorm(x),lwd = 2)\npar(mar = c(5.1, 4.1, 4.1, 2.1))\n\n#####################\npar(mar=c(2, 3.1, 2, 2.1))\nmidpts &lt;- barplot(VADeaths, col = gray(0.1 + seq(1,9,2)/11), names = rep(\"\", 4))\nmtext(sub(\" \", \"\\n\", colnames(VADeaths)),\n      at=midpts, side=1, line=0.5, cex=0.5)\ntext(rep(midpts, each=5), apply(VADeaths, 2, cumsum) - VADeaths/2,\n     VADeaths, \n     col=rep(c(\"white\", \"black\"), times=3:2), \n     cex=0.8)\npar(mar=c(5.1, 4.1, 4.1, 2.1))  \n#####################################\n# Piechart\npar(mar=c(0, 2, 1, 2), xpd=FALSE, cex=0.5)\npie.sales &lt;- c(0.12, 0.3, 0.26, 0.16, 0.04, 0.12)\nnames(pie.sales) &lt;- c(\"Blueberry\", \"Cherry\",\n                      \"Apple\", \"Boston Cream\", \"Other\", \"Vanilla\")\npie(pie.sales, col = gray(seq(0.3,1.0,length=6))) \n\n\n\n\n\n\n\n\n\n\n\n\ninstall.packages(\"ggplot2\")\n\nWarning: package 'ggplot2' is in use and will not be installed\n\nlibrary(ggplot2)\napple_data &lt;- data.frame(Product = c(\"iPhone\", \"iPad\", \"MacBook\", \"AirPods\"),\n                         Price = c(30, 35, 40, 17))\nggplot(apple_data, aes(x = Price, y = Product)) +\n  geom_bar(stat = \"Identity\", fill = \"skyblue\", width = 0.5) +\n  labs(x = \"Price\", y = \"Apple\", title = \"Price of Apple\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n*Assignment 1(Data Collection)*\nQ2.\na.¬† ¬†The survey is made up of a variety of blocks under which the questions encompass customer preferences, behaviours and demographics. It follows in a structured format where the questions are categorized by different categories of thew ell respondents such as movies, DVD, software and demographics.\nb.¬† ¬†The structure of the questionnaire includes questions scale based on LLikert scale, questions selection based on types of multiple choice and open ended. Similarly, the questionnaire topics include demographics, societal views on the media content, ideas for technologies, the patterns of purchase in terms of tools and how the respondents describe themselves in terms of age, sex and income.\nc.¬† ¬†The questions pose the problem of media users and their preference, then raise the issue of software interest and concern before shifting to the subject of buying behavior back and later turning to the demographic questions. This clear sequence helps in encouraging the respondents with the survey successful and ensures the most confidential information has been captured at such stage.\nQ4. The Diversity and Inclusiveness Survey features specific questions designed to assess various aspects of diversity and inclusivity, focusing on topics such as race, ethnicity, and gender. In contrast, the Movie Rental Survey is more general, allowing for the addition of a wide range of questions that may not directly relate to diversity or inclusiveness. This flexibility enables customization based on different research objectives, highlighting the tailored nature of the Diversity and Inclusiveness Survey compared to the open-ended approach of the Movie Rental Survey.\n*Assignment 2(Data Collection)*\nQ2. a) Analyzing Google Trends Data for ‚ÄúTrump,‚Äù ‚ÄúKamala Harris,‚Äù and ‚ÄúElection‚Äù Using the Google Trends Website: To analyze interest in ‚ÄúTrump,‚Äù ‚ÄúKamala Harris,‚Äù and ‚ÄúElection,‚Äù first visit the Google Trends website. Enter the keywords in the search bar and select a time frame for analysis. Once the data is displayed, you can download it as a CSV file. This method provides an intuitive interface for viewing trends, allowing for a manual inspection of dates and interest levels. You can visualize the data using software like Excel.\nb)¬† Using the gtrendsR Package in R: Alternatively, you can utilize the gtrendsR package in R for automated data retrieval. After installing and loading the package, use the gtrends function to collect data for the keywords. The resulting data frame includes detailed information on interest over time, making it easier to perform statistical analyses and visualizations directly in R.\nc)¬† Differences Between the Two Methods: The Google Trends website offers a user-friendly experience suitable for quick searches, but it requires manual data handling. In contrast, the gtrendsR package allows for automated collection of data, enabling more extensive analysis and visualization capabilities. Additionally, the package can provide finer granularity in data collection, making it preferable for users comfortable with coding and looking for deeper insights into trends over time.\n*Assignment 3(Data Collection)*\nQ1. over time similarity or differences among the presidents\nUsing quanteda, analyzing U.S. presidential speeches from 1826 to 1949 reveals shifts and continuities in presidential rhetoric. Word clouds of speeches highlight common themes, such as ‚Äúunion‚Äù and ‚Äúfreedom,‚Äù showing a consistent emphasis on unity, though the specific focus changes over time‚Äîfrom issues of slavery and states‚Äô rights in the 19th century to economic security and global democracy by the 1940s. Keyword analysis (KWIC) illustrates evolving terms like ‚ÄúAmerican,‚Äù which initially reflected nationalism and later expanded to embrace global leadership during World War II. By examining term frequency and keyness, we can distinguish between eras; for instance, Hoover‚Äôs speeches might emphasize individualism, while FDR‚Äôs focus on collective security and support reflects differing economic ideologies. Correspondence analysis helps to identify distinct themes, such as the rise of civil rights rhetoric post-Civil War or economic stability during the Great Depression. Together, these analyses show how presidents tailored their language to resonate with the evolving priorities and values of the American people.\n\n# Sample program for using quanteda for text modeling and analysis\n# Documentation: vignette(\"quickstart\", package = c(\"quanteda.textstats\", \"quanteda.textmodels\")\n# Website: https://quanteda.io/\n\nlibrary(quanteda)\n\nPackage version: 4.1.0\nUnicode version: 15.1\nICU version: 74.1\n\n\nParallel computing: 4 of 4 threads used.\n\n\nSee https://quanteda.io for tutorials and examples.\n\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(ggplot2)\n\n# Wordcloud\n# based on US presidential inaugural address texts, and metadata (for the corpus), from 1789 to present.\ndfm_inaug &lt;- corpus_subset(data_corpus_inaugural, Year &lt;= 1826) %&gt;% \n  tokens(remove_punct = TRUE) %&gt;% \n  tokens_remove(stopwords('english')) %&gt;% \n  dfm() %&gt;%\n  dfm_trim(min_termfreq = 10, verbose = FALSE)\n\nset.seed(100)\ntextplot_wordcloud(dfm_inaug)\n\n\n\n\n\n\n\ninaug_speech = data_corpus_inaugural\n\ncorpus_subset(data_corpus_inaugural, \n              President %in% c(\"Trump\", \"Obama\", \"Bush\")) %&gt;%\n  tokens(remove_punct = TRUE) %&gt;%\n  tokens_remove(stopwords(\"english\")) %&gt;%\n  dfm() %&gt;%\n  dfm_group(groups = President) %&gt;%\n  dfm_trim(min_termfreq = 5, verbose = FALSE) %&gt;%\n  textplot_wordcloud(comparison = TRUE)\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\ngovernment could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nwatching could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\npoverty could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nconfidence could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\naccept could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\ndefend could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nthroughout could not be fit on page. It will not be plotted.\n\n\n\n\n\n\n\n\ntextplot_wordcloud(dfm_inaug, min_count = 10,\n                   color = c('red', 'pink', 'green', 'purple', 'orange', 'blue'))\n\n\n\n\n\n\n\ndata_corpus_inaugural_subset &lt;- \n  corpus_subset(data_corpus_inaugural, Year &gt; 1949)\nkwic(tokens(data_corpus_inaugural_subset), pattern = \"american\") %&gt;%\n  textplot_xray()\n\n\n\n\n\n\n\ntextplot_xray(\n  kwic(tokens(data_corpus_inaugural_subset), pattern = \"american\"),\n  kwic(tokens(data_corpus_inaugural_subset), pattern = \"people\"),\n  kwic(tokens(data_corpus_inaugural_subset), pattern = \"communist\")\n  \n)\n\n\n\n\n\n\n\n## Why is the \"communist\" plot missing?\n##Because they didn't use this word in their speeches##\n\ntheme_set(theme_bw())\ng &lt;- textplot_xray(\n  kwic(tokens(data_corpus_inaugural_subset), pattern = \"american\"),\n  kwic(tokens(data_corpus_inaugural_subset), pattern = \"people\"),\n  kwic(tokens(data_corpus_inaugural_subset), pattern = \"communist\")\n)\ng + aes(color = keyword) + \n  scale_color_manual(values = c(\"blue\", \"red\", \"green\")) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nlibrary(quanteda.textstats)\nfeatures_dfm_inaug &lt;- textstat_frequency(dfm_inaug, n = 100)\n\n# Sort by reverse frequency order\nfeatures_dfm_inaug$feature &lt;- with(features_dfm_inaug, reorder(feature, -frequency))\n\nggplot(features_dfm_inaug, aes(x = feature, y = frequency)) +\n  geom_point() + \n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n\n\n\n\n# Get frequency grouped by president\nfreq_grouped &lt;- textstat_frequency(dfm(tokens(data_corpus_inaugural_subset)), \n                                   groups = data_corpus_inaugural_subset$President)\n\n# Filter the term \"american\"\nfreq_american &lt;- subset(freq_grouped, freq_grouped$feature %in% \"american\")  \n\nggplot(freq_american, aes(x = group, y = frequency)) +\n  geom_point() + \n  scale_y_continuous(limits = c(0, 14), breaks = c(seq(0, 14, 2))) +\n  xlab(NULL) + \n  ylab(\"Frequency\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n\n\n\n\ndfm_rel_freq &lt;- dfm_weight(dfm(tokens(data_corpus_inaugural_subset)), scheme = \"prop\") * 100\nhead(dfm_rel_freq)\n\nDocument-feature matrix of: 6 documents, 4,346 features (85.57% sparse) and 4 docvars.\n                 features\ndocs                      my    friends        ,    before          i\n  1953-Eisenhower 0.14582574 0.14582574 4.593511 0.1822822 0.10936930\n  1957-Eisenhower 0.20975354 0.10487677 6.345045 0.1573152 0.05243838\n  1961-Kennedy    0.19467878 0.06489293 5.451006 0.1297859 0.32446463\n  1965-Johnson    0.17543860 0.05847953 5.555556 0.2339181 0.87719298\n  1969-Nixon      0.28973510 0          5.546358 0.1241722 0.86920530\n  1973-Nixon      0.05012531 0.05012531 4.812030 0.2005013 0.60150376\n                 features\ndocs                   begin      the expression       of     those\n  1953-Eisenhower 0.03645643 6.234050 0.03645643 5.176814 0.1458257\n  1957-Eisenhower 0          5.977976 0          5.034085 0.1573152\n  1961-Kennedy    0.19467878 5.580792 0          4.218040 0.4542505\n  1965-Johnson    0          4.502924 0          3.333333 0.1754386\n  1969-Nixon      0          5.629139 0          3.890728 0.4552980\n  1973-Nixon      0          4.160401 0          3.408521 0.3007519\n[ reached max_nfeat ... 4,336 more features ]\n\nrel_freq &lt;- textstat_frequency(dfm_rel_freq, groups = dfm_rel_freq$President)\n\n# Filter the term \"american\"\nrel_freq_american &lt;- subset(rel_freq, feature %in% \"american\")  \n\nggplot(rel_freq_american, aes(x = group, y = frequency)) +\n  geom_point() + \n  scale_y_continuous(limits = c(0, 0.7), breaks = c(seq(0, 0.7, 0.1))) +\n  xlab(NULL) + \n  ylab(\"Relative frequency\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n\n\n\n\ndfm_weight_pres &lt;- data_corpus_inaugural %&gt;%\n  corpus_subset(Year &gt; 2000) %&gt;%\n  tokens(remove_punct = TRUE) %&gt;%\n  tokens_remove(stopwords(\"english\")) %&gt;%\n  dfm() %&gt;%\n  dfm_weight(scheme = \"prop\")\n\n# Calculate relative frequency by president\nfreq_weight &lt;- textstat_frequency(dfm_weight_pres, n = 10, \n                                  groups = dfm_weight_pres$President)\n\nggplot(data = freq_weight, aes(x = nrow(freq_weight):1, y = frequency)) +\n  geom_point() +\n  facet_wrap(~ group, scales = \"free\") +\n  coord_flip() +\n  scale_x_continuous(breaks = nrow(freq_weight):1,\n                     labels = freq_weight$feature) +\n  labs(x = NULL, y = \"Relative frequency\")\n\n\n\n\n\n\n\n# Only select speeches by Obama and Trump\npres_corpus &lt;- corpus_subset(data_corpus_inaugural, \n                             President %in% c(\"Obama\", \"Trump\"))\n\n# Create a dfm grouped by president\npres_dfm &lt;- tokens(pres_corpus, remove_punct = TRUE) %&gt;%\n  tokens_remove(stopwords(\"english\")) %&gt;%\n  tokens_group(groups = President) %&gt;%\n  dfm()\n\n# Calculate keyness and determine Trump as target group\nresult_keyness &lt;- textstat_keyness(pres_dfm, target = \"Trump\")\n\n# Plot estimated word keyness\ntextplot_keyness(result_keyness) \n\n\n\n\n\n\n\n# Plot without the reference text (in this case Obama)\ntextplot_keyness(result_keyness, show_reference = FALSE)\n\n\n\n\n\n\n\nlibrary(quanteda.textmodels)\n\n# Irish budget speeches from 2010 (data from quanteda.textmodels)\n# Transform corpus to dfm\ndata(data_corpus_irishbudget2010, package = \"quanteda.textmodels\")\nie_dfm &lt;- dfm(tokens(data_corpus_irishbudget2010))\n\n# Set reference scores\nrefscores &lt;- c(rep(NA, 4), 1, -1, rep(NA, 8))\n\n\n# Predict Wordscores model\nws &lt;- textmodel_wordscores(ie_dfm, y = refscores, smooth = 1)\n\n# Plot estimated word positions (highlight words and print them in red)\ntextplot_scale1d(ws,\n                 highlighted = c(\"minister\", \"have\", \"our\", \"budget\"), \n                 highlighted_color = \"red\")\n\n\n\n\n\n\n\n# Get predictions\npred &lt;- predict(ws, se.fit = TRUE)\n\n# Plot estimated document positions and group by \"party\" variable\ntextplot_scale1d(pred, margin = \"documents\",\n                 groups = docvars(data_corpus_irishbudget2010, \"party\"))\n\n\n\n\n\n\n\n# Plot estimated document positions using the LBG transformation and group by \"party\" variable\n\npred_lbg &lt;- predict(ws, se.fit = TRUE, rescaling = \"lbg\")\n\ntextplot_scale1d(pred_lbg, margin = \"documents\",\n                 groups = docvars(data_corpus_irishbudget2010, \"party\"))\n\n\n\n\n\n\n\n# Estimate Wordfish model\nlibrary(\"quanteda.textmodels\")\nwf &lt;- textmodel_wordfish(dfm(tokens(data_corpus_irishbudget2010)), dir = c(6, 5))\n\n# Plot estimated word positions\ntextplot_scale1d(wf, margin = \"features\", \n                 highlighted = c(\"government\", \"global\", \"children\", \n                                 \"bank\", \"economy\", \"the\", \"citizenship\",\n                                 \"productivity\", \"deficit\"), \n                 highlighted_color = \"red\")\n\n\n\n\n\n\n\n# Plot estimated document positions\ntextplot_scale1d(wf, groups = data_corpus_irishbudget2010$party)\n\n\n\n\n\n\n\n# Transform corpus to dfm\nie_dfm &lt;- dfm(tokens(data_corpus_irishbudget2010))\n\n# Run correspondence analysis on dfm\nca &lt;- textmodel_ca(ie_dfm)\n\n# Plot estimated positions and group by party\ntextplot_scale1d(ca, margin = \"documents\",\n                 groups = docvars(data_corpus_irishbudget2010, \"party\"))\n\n\n\n\n\n\n\n\nQ2. What is Wordfish?\nWordfish is a text scaling method used in computational analysis to position texts, like political speeches, on a single-dimensional spectrum based on word usage patterns. It creates a document-feature matrix, weights words by their distinguishing frequencies, and uses these patterns to map ideological or thematic differences without needing predefined labels. Ideal for large datasets, Wordfish effectively reveals text positioning but may oversimplify when texts cover multiple complex themes."
  },
  {
    "objectID": "coursework.html#data-visualization",
    "href": "coursework.html#data-visualization",
    "title": "Coursework",
    "section": "Data Visualization",
    "text": "Data Visualization"
  },
  {
    "objectID": "coursework.html#objective-create-graphics-with-r",
    "href": "coursework.html#objective-create-graphics-with-r",
    "title": "Coursework",
    "section": "Objective: Create graphics with R",
    "text": "Objective: Create graphics with R"
  },
  {
    "objectID": "coursework.html#title-fall-color",
    "href": "coursework.html#title-fall-color",
    "title": "Coursework",
    "section": "Title: Fall color",
    "text": "Title: Fall color"
  }
]